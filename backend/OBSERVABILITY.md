# Observability Guide

This document explains the logging and observability features in the Dr. Chaffee AI backend.

## Request Tracking

### Request ID (`request_id`)

Every HTTP request is assigned a unique 8-character ID by the `RequestIDMiddleware`.

- **Format**: `req-xxxxxxxx` (e.g., `req-a1b2c3d4`)
- **Source**: Auto-generated on each request
- **Purpose**: Correlate all log lines for a single request

### Session ID (`session_id`)

Optional browser session identifier sent via the `X-Session-ID` header.

- **Format**: 8-character hex string (e.g., `f3e2d1c0`)
- **Source**: Generated by frontend, stored in `localStorage`
- **Purpose**: Track user sessions across multiple requests

### Log Format

All RAG-related logs include a prefix with both IDs:

```
[req=a1b2c3d4 sess=f3e2d1c0] Search: query='carnivore diet' ...
```

If no session ID is provided, it shows as `sess=-`.

## Latency Metrics

The `/search` and `/answer` endpoints log detailed timing breakdowns:

### Search Endpoint

```
[req=xxx sess=yyy] Search: query='...' top_k=100 rerank=true
[req=xxx sess=yyy] SearchDone: results=25 embed_ms=45.2 search_ms=123.4 total_ms=180.5
```

| Metric | Description |
|--------|-------------|
| `embed_ms` | Time to generate query embedding (BGE-small-en-v1.5) |
| `search_ms` | Time for pgvector similarity search |
| `total_ms` | Total request duration |

### Answer Endpoint

```
[req=xxx sess=yyy] Answer: query='...' style=concise top_k=100 clips=20 rerank=true
[req=xxx sess=yyy] AnswerDone: clips=20 candidates=100 embed_ms=42.1 search_ms=98.3 llm_ms=2345.6 total_ms=2500.0 profile=default
```

| Metric | Description |
|--------|-------------|
| `embed_ms` | Query embedding generation |
| `search_ms` | Vector search + optional reranking |
| `llm_ms` | OpenAI API call duration |
| `total_ms` | Total request duration |
| `clips` | Number of clips used in LLM prompt |
| `candidates` | Total candidates from vector search |

## Caching

### Backend Caches

| Cache | TTL | Description |
|-------|-----|-------------|
| Embedding stats | 60s | Database embedding coverage stats |
| Search config | 60s | Search parameters (top_k, reranker, etc.) |
| RAG model catalog | 300s | Available LLM models |
| Embedding model catalog | 300s | Available embedding models |
| Tuning endpoints | 5s | General tuning API responses |

### Frontend Caches

| Cache | TTL | Description |
|-------|-----|-------------|
| Tuning metadata | 30s | RAG models, profiles, search config, instructions |

### Force Refresh

- **Backend**: Add `?refresh=true` query parameter to tuning endpoints
- **Frontend**: Click the refresh button (‚Üª) on tuning dashboard pages

## Debugging Tips

### Correlating Frontend and Backend Logs

1. Open browser DevTools Console
2. Note the session ID logged on page load: `üîç Dr Chaffee AI session: f3e2d1c0`
3. Search backend logs for `sess=f3e2d1c0` to find all requests from that session

### Identifying Slow Requests

1. Look for high `total_ms` values in logs
2. Check which component is slow:
   - High `embed_ms`: Embedding model issue
   - High `search_ms`: Database/pgvector issue
   - High `llm_ms`: OpenAI API latency

### Cache Issues

If data seems stale:
1. Use the refresh button on the tuning dashboard
2. Or add `?refresh=true` to the API URL
3. Backend caches auto-expire after their TTL

## Environment Variables

| Variable | Description |
|----------|-------------|
| `LOG_LEVEL` | Logging verbosity (default: INFO) |
| `DATABASE_URL` | PostgreSQL connection (passwords are masked in logs) |

## DSN Password Masking

Database connection strings are automatically masked in logs:

```
Before: postgresql://user:secret123@host:5432/db
After:  postgresql://user:***@host:5432/db
```
