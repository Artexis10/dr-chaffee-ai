#!/usr/bin/env python3
"""
Enhanced transcript fetching with speaker identification
Extends the existing TranscriptFetcher with Enhanced ASR capabilities
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

logger = logging.getLogger(__name__)

# Import existing transcript fetcher
from .transcript_fetch import TranscriptFetcher as BaseTranscriptFetcher
from .transcript_common import TranscriptSegment
from .segment_optimizer import SegmentOptimizer

class EnhancedTranscriptFetcher(BaseTranscriptFetcher):
    """
    Enhanced transcript fetcher with speaker identification capabilities
    Extends the existing TranscriptFetcher with Enhanced ASR integration
    """
    
    def __init__(self, 
                 yt_dlp_path: str = "yt-dlp", 
                 whisper_model: str = None, 
                 whisper_upgrade: str = None, 
                 ffmpeg_path: str = None, 
                 proxies: dict = None, 
                 api_key: str = None, 
                 credentials_path: str = None,
                 enable_preprocessing: bool = True,
                 # Enhanced ASR options
                 enable_speaker_id: bool = False,
                 voices_dir: str = None,
                 chaffee_min_sim: float = None,
                 guest_min_sim: float = None,
                 assume_monologue: bool = None,
                 # Audio storage options
                 store_audio_locally: bool = True,
                 audio_storage_dir: str = None,
                 production_mode: bool = False):
        
        # Initialize base class with audio storage parameters
        super().__init__(
            yt_dlp_path=yt_dlp_path,
            whisper_model=whisper_model,
            whisper_upgrade=whisper_upgrade,
            ffmpeg_path=ffmpeg_path,
            proxies=proxies,
            api_key=api_key,
            credentials_path=credentials_path,
            enable_preprocessing=enable_preprocessing,
            store_audio_locally=store_audio_locally,
            audio_storage_dir=audio_storage_dir,
            production_mode=production_mode
        )
        
        # Enhanced ASR configuration
        self.enable_speaker_id = enable_speaker_id or os.getenv('ENABLE_SPEAKER_ID', 'false').lower() == 'true'
        self.voices_dir = voices_dir or os.getenv('VOICES_DIR', 'voices')
        
        # Speaker ID thresholds
        self.chaffee_min_sim = chaffee_min_sim or float(os.getenv('CHAFFEE_MIN_SIM', '0.82'))
        self.guest_min_sim = guest_min_sim or float(os.getenv('GUEST_MIN_SIM', '0.82'))
        self.assume_monologue = assume_monologue if assume_monologue is not None else os.getenv('ASSUME_MONOLOGUE', 'true').lower() == 'true'
        
        # Audio storage is handled by parent class now
        
        # Initialize segment optimizer for better semantic search quality (uses env vars)
        self.enable_segment_optimization = os.getenv('ENABLE_SEGMENT_OPTIMIZATION', 'true').lower() == 'true'
        self.segment_optimizer = SegmentOptimizer()  # Uses environment variables for configuration
        
        # Add alias for compatibility
        self.downloader = self.audio_downloader
        
        # Lazy-loaded Enhanced ASR components
        self._enhanced_asr = None
        self._voice_enrollment = None
        
        logger.info(f"Enhanced Transcript Fetcher initialized (speaker_id={self.enable_speaker_id}, audio_storage={self.store_audio_locally})")
    
    def _get_enhanced_asr(self):
        """Lazy load Enhanced ASR system"""
        if self._enhanced_asr is None and self.enable_speaker_id:
            try:
                from .enhanced_asr import EnhancedASR, EnhancedASRConfig
                
                # Create config with current settings
                config = EnhancedASRConfig()
                config.chaffee_min_sim = self.chaffee_min_sim
                config.guest_min_sim = self.guest_min_sim
                config.assume_monologue = self.assume_monologue
                config.whisper_model = self.whisper_model
                config.voices_dir = self.voices_dir
                
                self._enhanced_asr = EnhancedASR(config)
                logger.info("Enhanced ASR system loaded")
                
            except ImportError as e:
                logger.warning(f"Enhanced ASR not available: {e}")
                logger.info("Install dependencies: pip install whisperx pyannote.audio speechbrain")
                self.enable_speaker_id = False
        
        return self._enhanced_asr
    
    def _get_voice_enrollment(self):
        """Lazy load voice enrollment system"""
        if self._voice_enrollment is None:
            try:
                from .voice_enrollment_optimized import VoiceEnrollment
                self._voice_enrollment = VoiceEnrollment(voices_dir=self.voices_dir)
            except ImportError as e:
                logger.warning(f"Voice enrollment not available: {e}")
        
        return self._voice_enrollment
    
    def _check_speaker_profiles_available(self) -> bool:
        """Check if any speaker profiles are available"""
        try:
            enrollment = self._get_voice_enrollment()
            if enrollment:
                profiles = enrollment.list_profiles()
                return len(profiles) > 0
        except Exception as e:
            logger.warning(f"Failed to check speaker profiles: {e}")
        
        return False
    
    def _convert_enhanced_result_to_segments(self, enhanced_result) -> Tuple[List[TranscriptSegment], Dict[str, Any]]:
        """Convert Enhanced ASR result to TranscriptSegment format"""
        try:
            segments = []
            metadata = enhanced_result.metadata.copy()
            
            # Convert segments with speaker information
            for segment_data in enhanced_result.segments:
                # Create TranscriptSegment with speaker info embedded in text
                text = segment_data['text'].strip()
                
                # Create TranscriptSegment with speaker information
                speaker_label = segment_data.get('speaker', 'Guest')
                if speaker_label == enhanced_result.metadata.get('unknown_label', 'Unknown'):
                    speaker_label = 'Guest'
                
                # Extract voice embedding
                voice_emb = segment_data.get('voice_embedding', None)
                
                segment = TranscriptSegment(
                    start=segment_data['start'],
                    end=segment_data['end'],
                    text=text,
                    speaker_label=speaker_label,
                    speaker_confidence=segment_data.get('speaker_confidence', None),
                    avg_logprob=segment_data.get('avg_logprob', None),
                    compression_ratio=segment_data.get('compression_ratio', None),
                    no_speech_prob=segment_data.get('no_speech_prob', None),
                    temperature_used=segment_data.get('temperature_used', 0.0),
                    re_asr=segment_data.get('re_asr', False),
                    is_overlap=segment_data.get('is_overlap', False),
                    needs_refinement=segment_data.get('needs_refinement', False),
                    voice_embedding=voice_emb
                    )
                
                segments.append(segment)
            
            # DIAGNOSTIC: Check voice embedding coverage
            voice_emb_count = sum(1 for s in segments if s.voice_embedding is not None)
            logger.info(f"ðŸ“Š Voice embedding coverage: {voice_emb_count}/{len(segments)} segments ({voice_emb_count/len(segments)*100:.1f}%)")
            
            # Add enhanced ASR metadata
            metadata.update({
                'enhanced_asr_used': True,
                'speaker_identification': True,
                'processing_method': enhanced_result.metadata.get('method', 'enhanced_asr')
            })
            
            # Include summary if available
            if 'summary' in enhanced_result.metadata:
                summary = enhanced_result.metadata['summary']
                metadata.update({
                    'chaffee_percentage': summary.get('chaffee_percentage', 0.0),
                    'speaker_distribution': summary.get('speaker_time_percentages', {}),
                    'unknown_segments': summary.get('unknown_segments', 0)
                })
            
            # Optimize segments for better semantic search quality
            logger.info(f"Optimizing {len(segments)} segments for semantic search")
            optimized_segments = self.segment_optimizer.optimize_segments(segments)
            
            # Convert back to TranscriptSegment objects
            final_segments = []
            for opt_seg in optimized_segments:
                transcript_seg = TranscriptSegment(
                    start=opt_seg.start,
                    end=opt_seg.end,
                    text=opt_seg.text,
                    speaker_label=opt_seg.speaker_label,
                    speaker_confidence=opt_seg.speaker_confidence,
                    avg_logprob=opt_seg.avg_logprob,
                    compression_ratio=opt_seg.compression_ratio,
                    no_speech_prob=opt_seg.no_speech_prob,
                    temperature_used=opt_seg.temperature_used,
                    re_asr=opt_seg.re_asr,
                    is_overlap=opt_seg.is_overlap,
                    needs_refinement=opt_seg.needs_refinement,
                    voice_embedding=opt_seg.voice_embedding
                )
                final_segments.append(transcript_seg)
            
            # Update metadata with optimization info
            metadata.update({
                'segment_optimization': True,
                'original_segment_count': len(segments),
                'optimized_segment_count': len(final_segments),
                'optimization_reduction': len(segments) - len(final_segments)
            })
            
            logger.info(f"Segment optimization complete: {len(segments)} â†’ {len(final_segments)} segments")
            return final_segments, metadata
            
        except Exception as e:
            logger.error(f"Failed to convert Enhanced ASR result: {e}")
            # Fallback to basic segments WITH speaker labels (prevent NULL labels)
            segments = []
            for segment_data in enhanced_result.segments:
                segment = TranscriptSegment(
                    start=segment_data['start'],
                    end=segment_data['end'],
                    text=segment_data['text'].strip(),
                    speaker_label='Chaffee'  # Default to Chaffee (capitalized)
                )
                segments.append(segment)
            
            logger.warning(f"Enhanced ASR conversion failed, using {len(segments)} segments with default speaker labels")
            return segments, {'enhanced_asr_used': True, 'conversion_error': str(e), 'fallback_speaker_label': 'Chaffee'}
    
    def fetch_transcript_with_speaker_id(
        self, 
        video_id_or_path: str, 
        max_duration_s: Optional[int] = None,
        force_enhanced_asr: bool = False,
        cleanup_audio: bool = True,
        enable_silence_removal: bool = False,
        is_local_file: bool = False,
        allow_youtube_captions: bool = False,
        segments_db=None,
        video_id: Optional[str] = None
    ) -> Tuple[Optional[List[TranscriptSegment]], str, Dict[str, Any]]:
        """
        Fetch transcript with MANDATORY speaker identification
        
        CRITICAL: This pipeline requires speaker identification for accurate attribution.
        YouTube captions are DISABLED by default because they:
        - Bypass speaker diarization
        - Bypass Chaffee voice profile matching
        - Cannot distinguish between Chaffee and guests
        - Risk misattributing guest statements to Dr. Chaffee
        
        Args:
            video_id_or_path: YouTube video ID or local audio/video file path
            max_duration_s: Maximum duration for processing
            force_enhanced_asr: Force Enhanced ASR even if speaker profiles unavailable
            cleanup_audio: Clean up temporary audio files
            enable_silence_removal: Enable audio preprocessing
            is_local_file: True if video_id_or_path is a local file path
            allow_youtube_captions: EXPLICITLY allow YouTube captions (NOT RECOMMENDED)
                                   Only use if speaker ID is not required
            
        Returns:
            (segments, method, metadata) where method indicates processing used
        """
        metadata = {"video_id": video_id_or_path, "preprocessing_flags": {}, "is_local_file": is_local_file}
        
        # MANDATORY: Speaker identification is required for this pipeline
        # YouTube captions bypass our sophisticated Chaffee identification system
        if self.enable_speaker_id:
            logger.info(f"ðŸŽ¯ Speaker ID enabled - Enhanced ASR REQUIRED for {video_id_or_path}")
            force_enhanced_asr = True  # Override to force Enhanced ASR
        else:
            logger.warning(f"âš ï¸ Speaker ID DISABLED for {video_id_or_path} - will use standard Whisper")
        
        # Check if Enhanced ASR is available and should be used
        use_enhanced_asr = (
            force_enhanced_asr or 
            (self.enable_speaker_id and self._check_speaker_profiles_available())
        )
        
        logger.info(f"DEBUG: enable_speaker_id={self.enable_speaker_id}, force_enhanced_asr={force_enhanced_asr}, use_enhanced_asr={use_enhanced_asr}")
        
        if use_enhanced_asr:
            enhanced_asr = self._get_enhanced_asr()
            logger.info(f"DEBUG: enhanced_asr object = {enhanced_asr}")
            if not enhanced_asr:
                logger.error("âŒ Enhanced ASR required but not available - speaker ID cannot be performed")
                if self.enable_speaker_id:
                    # FAIL HARD - do not fall back to YouTube captions when speaker ID is required
                    metadata['error'] = 'Enhanced ASR unavailable - speaker identification required'
                    return None, 'failed', metadata
                logger.warning("Enhanced ASR requested but not available, falling back to standard method")
                use_enhanced_asr = False
        
        # YouTube captions are OPT-IN only, and NEVER used when speaker ID is enabled
        # This prevents misattribution of guest statements to Dr. Chaffee
        if allow_youtube_captions and not self.enable_speaker_id and not is_local_file and not force_enhanced_asr:
            logger.warning("âš ï¸  YouTube captions allowed - NO SPEAKER IDENTIFICATION will be performed")
            youtube_segments = self.fetch_youtube_transcript(video_id_or_path)
            if youtube_segments:
                metadata.update({"source": "youtube", "segment_count": len(youtube_segments), "speaker_id_bypassed": True})
                logger.warning(f"Using YouTube captions for {video_id_or_path} - segments will have NO speaker labels")
                return youtube_segments, 'youtube', metadata
        
        # If we have speaker profiles and Enhanced ASR available, use it
        if use_enhanced_asr and self._check_speaker_profiles_available():
            logger.info(f"Using Enhanced ASR with speaker identification for {video_id_or_path}")
            
            try:
                # Handle audio source - local file or YouTube download
                if is_local_file or os.path.exists(video_id_or_path):
                    # Local file path
                    audio_path = video_id_or_path
                    if not os.path.exists(audio_path):
                        logger.error(f"Audio file not found: {audio_path}")
                        return None, 'failed', metadata
                    downloaded_audio = False
                elif len(video_id_or_path) == 11 and video_id_or_path.replace('-', '').replace('_', '').isalnum():
                    # YouTube video ID - need to download audio first
                    audio_path = self._download_audio_for_enhanced_asr(video_id_or_path)
                    if not audio_path:
                        logger.error("Failed to download audio for Enhanced ASR")
                        return self._fallback_to_standard_whisper(video_id_or_path, metadata)
                    downloaded_audio = True
                else:
                    logger.error(f"Invalid video ID or file path: {video_id_or_path}")
                    return None, 'failed', metadata
                
                # Process with Enhanced ASR
                enhanced_asr = self._get_enhanced_asr()
                
                # CRITICAL: Pass segments_db and video_id for voice embedding caching
                if segments_db is not None:
                    enhanced_asr.segments_db = segments_db
                if video_id is not None:
                    enhanced_asr.video_id = video_id
                    logger.info(f"ðŸ”‘ Voice embedding cache enabled for video: {video_id}")
                
                result = enhanced_asr.transcribe_with_speaker_id(audio_path)
                
                if result:
                    segments, enhanced_metadata = self._convert_enhanced_result_to_segments(result)
                    metadata.update(enhanced_metadata)
                    
                    # Handle cleanup and storage
                    if downloaded_audio:
                        if self.store_audio_locally:
                            # Store audio permanently
                            stored_path = self._store_audio_permanently(audio_path, video_id_or_path)
                            if stored_path:
                                metadata["stored_audio_path"] = str(stored_path)
                                logger.info(f"Audio stored at: {stored_path}")
                        
                        if cleanup_audio:
                            # Only cleanup if not storing or storage failed
                            if not self.store_audio_locally or not metadata.get("stored_audio_path"):
                                try:
                                    os.unlink(audio_path)
                                    logger.debug(f"Cleaned up temporary audio: {audio_path}")
                                except Exception as e:
                                    logger.warning(f"Failed to cleanup temporary audio: {e}")
                    
                    method = 'enhanced_asr'
                    if result.metadata.get('monologue_fast_path'):
                        method = 'enhanced_asr_monologue'
                    
                    logger.info(f"Enhanced ASR completed: {len(segments)} segments with speaker ID")
                    return segments, method, metadata
                else:
                    logger.warning("Enhanced ASR failed, falling back to standard Whisper")
                    return self._fallback_to_standard_whisper(video_id_or_path, metadata)
                    
            except Exception as e:
                logger.error(f"Enhanced ASR processing failed: {e}")
                return self._fallback_to_standard_whisper(video_id_or_path, metadata)
        
        # Fallback to standard transcript fetching
        logger.info("Using standard transcript fetching (no speaker ID)")
        
        if is_local_file:
            # For local files, use Whisper directly
            return self._transcribe_local_file(
                video_id_or_path, 
                max_duration_s=max_duration_s,
                enable_silence_removal=enable_silence_removal,
                metadata=metadata
            )
        else:
            return super().fetch_transcript(
                video_id_or_path, 
                max_duration_s=max_duration_s,
                force_whisper=force_enhanced_asr,
                cleanup_audio=cleanup_audio,
                enable_silence_removal=enable_silence_removal
            )
    
    def _download_audio_for_enhanced_asr(self, video_id: str) -> Optional[str]:
        """Download audio file for Enhanced ASR processing"""
        try:
            import tempfile
            import subprocess
            
            # Create temporary directory for download
            temp_dir = tempfile.mkdtemp()
            output_template = os.path.join(temp_dir, f'{video_id}.%(ext)s')
            
            # Use yt-dlp with android client to avoid SABR streaming issues
            cmd = [
                self.yt_dlp_path,
                '--format', 'bestaudio[ext=m4a]/bestaudio[ext=webm]/bestaudio/best',  # Prioritize best quality, prefer lossless formats
                '--no-playlist', 
                '--ignore-errors',
                # Use android client to avoid SABR streaming (web_safari causes errors)
                '--extractor-args', 'youtube:player_client=android',
                '-4',  # Force IPv4 (fixes many 403s)
                '--retry-sleep', '2',  # Pause between retries  
                '--retries', '10',  # More retries
                '--fragment-retries', '10',
                '--sleep-requests', '1',  # Reduced sleep for faster downloads
                '--min-sleep-interval', '1',  # Min sleep time
                '--max-sleep-interval', '3',  # Max sleep time (reduced for speed)
                '--socket-timeout', '30',  # Reduced timeout
                '--user-agent', 'Mozilla/5.0 (Linux; Android 13) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                '--referer', 'https://www.youtube.com/',
                '-o', output_template,
                f'https://www.youtube.com/watch?v={video_id}'
            ]
            
            # Add GPT-5's browser cookie solution (Firefox most reliable)
            # Check for cookies.txt file first (manual export)
            cookies_file = os.path.join(os.path.dirname(output_template), 'cookies.txt')
            if os.path.exists('cookies.txt'):
                cmd.extend(['--cookies', 'cookies.txt'])
                logger.info("Using manual cookies.txt file")
            else:
                # Try Firefox browser cookies (GPT-5 recommendation: most reliable)
                cmd.extend(['--cookies-from-browser', 'firefox'])
                logger.info("Attempting to use Firefox cookies (GPT-5 recommended)")
            
            if self.ffmpeg_path:
                cmd.extend(['--ffmpeg-location', self.ffmpeg_path])
            
            if self.proxies:
                if isinstance(self.proxies, dict) and 'http' in self.proxies:
                    cmd.extend(['--proxy', self.proxies['http']])
                elif isinstance(self.proxies, str):
                    cmd.extend(['--proxy', self.proxies])
            
            logger.info(f"Running yt-dlp command: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            
            logger.info(f"yt-dlp stdout: {result.stdout}")
            if result.stderr:
                logger.warning(f"yt-dlp stderr: {result.stderr}")
            
            if result.returncode == 0:
                # Find the actual output file (various audio formats)
                for ext in ['.webm', '.mp4', '.m4a', '.mp3', '.wav', '.opus']:
                    potential_path = os.path.join(temp_dir, f"{video_id}{ext}")
                    if os.path.exists(potential_path):
                        logger.info(f"Audio downloaded successfully: {potential_path}")
                        return potential_path
                
                # List all files in temp directory for debugging
                files_in_dir = os.listdir(temp_dir)
                logger.error(f"Audio download succeeded but file not found. Files in {temp_dir}: {files_in_dir}")
                
                # Clean up temp directory
                import shutil
                shutil.rmtree(temp_dir, ignore_errors=True)
                return None
            else:
                logger.error(f"Audio download failed: {result.stderr}")
                # Clean up temp directory
                import shutil
                shutil.rmtree(temp_dir, ignore_errors=True)
                return None
                
        except Exception as e:
            logger.error(f"Failed to download audio for Enhanced ASR: {e}")
            # Clean up temp directory if it exists
            if 'temp_dir' in locals():
                import shutil
                shutil.rmtree(temp_dir, ignore_errors=True)
            return None
    
    def _store_audio_permanently(self, temp_audio_path: str, video_id: str) -> Optional[Path]:
        """Store downloaded audio file permanently using parent class method"""
        return super()._store_audio_permanently(Path(temp_audio_path), video_id)
    
    def _transcribe_local_file(
        self, 
        file_path: str, 
        max_duration_s: Optional[int] = None,
        enable_silence_removal: bool = False,
        metadata: Dict[str, Any] = None
    ) -> Tuple[Optional[List[TranscriptSegment]], str, Dict[str, Any]]:
        """Transcribe a local audio/video file using Whisper"""
        if metadata is None:
            metadata = {}
        
        logger.info(f"Transcribing local file with Whisper: {file_path}")
        
        try:
            # Use parent class method but with local file
            segments, whisper_metadata = self.transcribe_with_whisper_fallback(
                Path(file_path),
                model_name=self.whisper_model,
                enable_silence_removal=enable_silence_removal
            )
            
            if segments:
                metadata.update(whisper_metadata)
                metadata["source"] = "local_file"
                metadata["file_path"] = file_path
                return segments, "whisper_local", metadata
            else:
                metadata["error"] = "transcription_failed"
                return None, 'failed', metadata
                
        except Exception as e:
            logger.error(f"Local file transcription failed: {e}")
            metadata["error"] = str(e)
            return None, 'failed', metadata
    
    def _fallback_to_standard_whisper(self, video_id_or_path: str, metadata: Dict[str, Any]) -> Tuple[Optional[List[TranscriptSegment]], str, Dict[str, Any]]:
        """Fallback to standard Whisper processing"""
        logger.info("Falling back to standard Whisper transcription")
        
        try:
            # Check if it's a local file
            if metadata.get("is_local_file") or os.path.exists(video_id_or_path):
                return self._transcribe_local_file(
                    video_id_or_path,
                    metadata=metadata
                )
            else:
                segments, method, whisper_metadata = super().fetch_transcript(
                    video_id_or_path, 
                    force_whisper=True,
                    cleanup_audio=True
                )
                
                # Merge metadata
                metadata.update(whisper_metadata)
                metadata['enhanced_asr_fallback'] = True
                
                return segments, "whisper", metadata
            
        except Exception as e:
            logger.error(f"Standard Whisper fallback also failed: {e}")
            metadata['error'] = str(e)
            return None, 'failed', metadata
    
    def enroll_speaker_from_video(
        self, 
        video_id: str, 
        speaker_name: str, 
        overwrite: bool = False,
        min_duration: float = 30.0
    ) -> bool:
        """
        Enroll a speaker using audio from a YouTube video
        
        Args:
            video_id: YouTube video ID
            speaker_name: Name for the speaker profile
            overwrite: Whether to overwrite existing profile
            min_duration: Minimum audio duration required
            
        Returns:
            True if enrollment successful, False otherwise
        """
        try:
            enrollment = self._get_voice_enrollment()
            if not enrollment:
                logger.error("Voice enrollment system not available")
                return False
            
            youtube_url = f"https://www.youtube.com/watch?v={video_id}"
            
            profile = enrollment.enroll_speaker(
                name=speaker_name,
                audio_sources=[youtube_url],
                overwrite=overwrite,
                min_duration=min_duration
            )
            
            return profile is not None
            
        except Exception as e:
            logger.error(f"Failed to enroll speaker from video {video_id}: {e}")
            return False
    
    def get_speaker_profiles(self) -> List[str]:
        """Get list of available speaker profiles"""
        try:
            enrollment = self._get_voice_enrollment()
            if enrollment:
                return enrollment.list_profiles()
        except Exception as e:
            logger.warning(f"Failed to get speaker profiles: {e}")
        
        return []
    
    def get_enhanced_asr_status(self) -> Dict[str, Any]:
        """Get status of Enhanced ASR system"""
        status = {
            'enabled': self.enable_speaker_id,
            'available': False,
            'voice_profiles': [],
            'config': {
                'chaffee_min_sim': self.chaffee_min_sim,
                'guest_min_sim': self.guest_min_sim,
                'assume_monologue': self.assume_monologue,
                'voices_dir': self.voices_dir
            }
        }
        
        if self.enable_speaker_id:
            try:
                enhanced_asr = self._get_enhanced_asr()
                status['available'] = enhanced_asr is not None
                status['voice_profiles'] = self.get_speaker_profiles()
            except Exception as e:
                status['error'] = str(e)
        
        return status

def main():
    """CLI for testing enhanced transcript fetching"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Enhanced transcript fetching with speaker ID')
    parser.add_argument('video_id', help='YouTube video ID or audio file path')
    parser.add_argument('--enable-speaker-id', action='store_true', help='Enable speaker identification')
    parser.add_argument('--force-enhanced-asr', action='store_true', help='Force Enhanced ASR usage')
    parser.add_argument('--voices-dir', default='voices', help='Voice profiles directory')
    parser.add_argument('--output', help='Output file for results')
    parser.add_argument('--format', choices=['segments', 'json', 'summary'], default='segments')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose logging')
    
    args = parser.parse_args()
    
    # Setup logging
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=level, format='%(levelname)s: %(message)s')
    
    # Initialize enhanced fetcher
    fetcher = EnhancedTranscriptFetcher(
        enable_speaker_id=args.enable_speaker_id,
        voices_dir=args.voices_dir
    )
    
    # Show status
    if args.verbose:
        status = fetcher.get_enhanced_asr_status()
        print(f"Enhanced ASR Status: {status}")
    
    # Fetch transcript
    segments, method, metadata = fetcher.fetch_transcript_with_speaker_id(
        args.video_id,
        force_enhanced_asr=args.force_enhanced_asr
    )
    
    if segments:
        print(f"\nTranscript fetched using {method} ({len(segments)} segments)")
        
        if args.format == 'segments':
            for i, segment in enumerate(segments[:5]):
                speaker_info = ""
                if hasattr(segment, 'metadata') and segment.metadata:
                    if 'speaker' in segment.metadata:
                        speaker = segment.metadata['speaker']
                        conf = segment.metadata.get('speaker_confidence', 0.0)
                        speaker_info = f" [{speaker}: {conf:.2f}]"
                
                print(f"  {segment.start:.1f}s - {segment.end:.1f}s: {segment.text}{speaker_info}")
            
            if len(segments) > 5:
                print(f"  ... and {len(segments) - 5} more segments")
        
        elif args.format == 'json':
            import json
            output_data = {
                'method': method,
                'segments': [{'start': s.start, 'end': s.end, 'text': s.text} for s in segments],
                'metadata': metadata
            }
            print(json.dumps(output_data, indent=2))
        
        elif args.format == 'summary':
            print(f"\nProcessing Summary:")
            print(f"Method: {method}")
            print(f"Segments: {len(segments)}")
            if 'chaffee_percentage' in metadata:
                print(f"Chaffee: {metadata['chaffee_percentage']:.1f}%")
            if 'speaker_distribution' in metadata:
                print("Speaker distribution:")
                for speaker, percentage in metadata['speaker_distribution'].items():
                    print(f"  {speaker}: {percentage:.1f}%")
        
        # Save output if requested
        if args.output:
            if args.format == 'json':
                with open(args.output, 'w') as f:
                    json.dump(output_data, f, indent=2)
            else:
                with open(args.output, 'w') as f:
                    for segment in segments:
                        f.write(f"{segment.start:.1f}\t{segment.end:.1f}\t{segment.text}\n")
            print(f"Results saved to: {args.output}")
    
    else:
        print("Failed to fetch transcript")
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main())
