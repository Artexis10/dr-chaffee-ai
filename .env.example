# =============================================================================
# RTX 5080 OPTIMIZED DR. CHAFFEE INGESTION PIPELINE CONFIGURATION
# =============================================================================

# Database Configuration
DATABASE_URL=postgresql://postgres:password@localhost:5432/askdrchaffee
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
POSTGRES_DB=askdrchaffee

# Application Security
# Set APP_PASSWORD to require password authentication for the frontend
# Leave empty or unset to disable password protection
APP_PASSWORD=

# YouTube Configuration
YOUTUBE_CHANNEL_URL=https://www.youtube.com/@anthonychaffeemd
YOUTUBE_API_KEY=

# RTX 5080 Whisper Configuration (Production Optimized)
WHISPER_MODEL=distil-large-v3
WHISPER_COMPUTE=int8_float16
WHISPER_VAD=false
BEAM_SIZE=5
TEMPERATURE=0.0
MAX_AUDIO_DURATION=3600

# Parallel Whisper Models
# GPU (CUDA): Can use 1 or 2 models
#   Option 1 (Recommended): 1 model + high ASR_WORKERS (8-12)
#     - GPU batches internally via CUDA streams
#     - Simpler, less overhead
#   Option 2: 2 models + moderate ASR_WORKERS (4-6)
#     - Each model handles different videos
#     - Uses more VRAM (12-14GB vs 6-7GB)
#     - May improve utilization if workers are bottlenecked
# CPU: Set to 2-4 (bypasses Python GIL for true multi-threading)
WHISPER_PARALLEL_MODELS=1  # Start with 1, try 2 if GPU util < 70%
WHISPER_DEVICE=cuda  # 'cuda' for GPU, 'cpu' for production

# RTX 5080 Concurrency Settings (Optimized for 80-90% GPU)
IO_WORKERS=24        # Download queue
ASR_WORKERS=8        # Start with 8, increase to 12 if GPU util < 80%
                     # With 1 model: Use 8-12 workers (GPU batches via CUDA)
                     # With 2 models: Use 4-6 workers (round-robin between models)
DB_WORKERS=12
BATCH_SIZE=1024      # Large batches for GPU efficiency

# Segment Optimization (for better semantic search)
SEGMENT_MIN_CHARS=1100    # Minimum chars for optimal segments
SEGMENT_MAX_CHARS=1400    # Maximum chars before splitting
SEGMENT_MAX_GAP_SECONDS=5.0      # Merge across pauses up to 5 seconds
SEGMENT_MAX_MERGE_DURATION=120.0  # Allow segments up to 2 minutes long

# Speaker Identification
ENABLE_SPEAKER_ID=true
VOICES_DIR=voices
CHAFFEE_MIN_SIM=0.62
GUEST_MIN_SIM=0.82
ATTR_MARGIN=0.05
ASSUME_MONOLOGUE=true
USE_SIMPLE_DIARIZATION=false
AUTO_BOOTSTRAP_CHAFFEE=true

# Performance Optimization
CHUNK_DURATION_SECONDS=45
ENABLE_FAST_PATH=true
CHAFFEE_ONLY_STORAGE=false

# Segment Optimization for RAG (GPT-5 recommended settings)
ENABLE_SEGMENT_OPTIMIZATION=true
SEGMENT_MIN_CHARS=1100          # Target minimum for good context
SEGMENT_MAX_CHARS=1400          # Target maximum (hard cap: 1800)
SEGMENT_HARD_CAP_CHARS=1800     # Absolute maximum
SEGMENT_OVERLAP_CHARS=250       # Overlap between segments (220-300)
SEGMENT_MAX_GAP_SECONDS=0.75    # Merge across pauses â‰¥750ms
SEGMENT_MAX_MERGE_DURATION=90.0 # 60-90 seconds per segment

# =============================================================================
# EMBEDDING CONFIGURATION - Choose your model profile
# =============================================================================
# Options: 'quality' (GTE-Qwen2-1.5B) or 'speed' (BGE-Small)
EMBEDDING_PROFILE=quality

# Profile: 'quality' - GTE-Qwen2-1.5B (1536-dim, 20-30 texts/sec, best quality)
# Profile: 'speed' - BGE-Small (384-dim, 1500-2000 texts/sec, 60-80x faster)

# Advanced: Override profile settings (leave commented to use profile defaults)
# EMBEDDING_PROVIDER=local
# EMBEDDING_MODEL=Alibaba-NLP/gte-Qwen2-1.5B-instruct
# EMBEDDING_DIMENSIONS=1536
# EMBEDDING_DEVICE=cuda
# EMBEDDING_BATCH_SIZE=256

# Reranker Configuration (Optional - improves BGE-Small quality by 3-5%)
ENABLE_RERANKER=false  # Set to 'true' for BGE-Small to recover quality loss
RERANK_TOP_K=200  # Retrieve this many candidates for reranking
RETURN_TOP_K=20   # Return this many after reranking
RERANK_BATCH_SIZE=64

# yt-dlp Configuration
YTDLP_BIN=yt-dlp
YTDLP_OPTS=--sleep-requests 1 --max-sleep-interval 3 --retries 10 --fragment-retries 10 --socket-timeout 20
YTDLP_DOWNLOAD_SEMAPHORE=20  # Max concurrent downloads (10-30 recommended)

# Processing Settings
SKIP_SHORTS=true
NEWEST_FIRST=true
RERANK_ENABLED=true
CLEANUP_AUDIO_AFTER_PROCESSING=true

# HuggingFace Configuration (for model downloads)
HUGGINGFACE_HUB_TOKEN=your_huggingface_token_here

# OpenAI Configuration (Optional - for embeddings or summarization)
OPENAI_API_KEY=your_openai_api_key_here
SUMMARIZER_MODEL=gpt-3.5-turbo

# Answer Mode Configuration
ANSWER_ENABLED=true
ANSWER_TOPK=40
ANSWER_TTL_HOURS=336
SUMMARIZER_MODEL=gpt-3.5-turbo
ANSWER_STYLE_DEFAULT=concise

# Audio Storage Cleanup
# Set to 'true' to automatically delete audio files after processing embeddings
CLEANUP_AUDIO_AFTER_PROCESSING=false
